{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the text and corresponding evidence, identify and flag any factual errors. Return 1 if very likely factual errors are found, or return 0 if very likely no factual errors are found. Only provide the number without any additional information.\n",
    "\n",
    "# Text: {text}\n",
    "# Evidence: {evidence}\n",
    "\n",
    "\n",
    "\n",
    "# Identify and flag any factual errors in the given text and corresponding evidence. Return 1 if factual errors are found, or return 0 if no factual errors are found.Only provide the number without any additional information.\n",
    "\n",
    "# Text: {text}\n",
    "# Evidence: {evidence}\n",
    "\n",
    "\n",
    "\n",
    "# Given a piece of text and corresponding evidence, your task is to identify and flag any factual errors in the text. If the text is factually accurate based on the provided evidence, return a value between 0 and 1 (inclusive) in increments of 0.2, representing the level of confidence in its accuracy. It is sufficient to return a single number without any other information.\n",
    "\n",
    "# 0: No factual errors found.\n",
    "# 0.2: Slight doubts about the accuracy of the text.\n",
    "# 0.4: Some factual errors detected.\n",
    "# 0.6: Moderate doubts about the accuracy of the text.\n",
    "# 0.8: Significant factual errors identified.\n",
    "# 1: Text contains multiple factual errors.\n",
    "\n",
    "# Text: {text}\n",
    "# Evidence: {evidence}\n",
    "\n",
    "\n",
    "\n",
    "# Given a piece of text and corresponding evidence, your task is to identify and flag any factual errors in the text. If the text is factually accurate based on the provided evidence, return a confidence score between 0 and 1. A score closer to 1 indicates the presence of multiple factual errors, while a score closer to 0 indicates no factual errors found. Provide only the confidence score without additional information.\n",
    "\n",
    "# Text: {text}\n",
    "# Evidence: {evidence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_mt='This is a machine translation task. Given a standard translation, and a model output translation, determine if the model output is subject to hallucination.\\n\\\n",
    "{demo}\\n\\n\\\n",
    "your task:\\n\\\n",
    "standard translation: {Evidence}\\n\\\n",
    "model output translation: {Text}\\n\\n\\\n",
    "The criteria for judging are as follows:\\n\\\n",
    "Check if the model output translation is fluent and answers the question.\\n\\\n",
    "Compare the model output translation with correct examples. If inconsistencies are found or it can\\'t be inferred from the standard translation, it\\'s likely hallucination.\\n\\\n",
    "If the model output translation aligns with the standard translation or has a similar meaning, it\\'s likely not hallucination.\\n\\\n",
    "If the standard translation is \"unanswerable\" and the model output translation is \"I don\\'t know,\" it\\'s likely not hallucination.\\n\\\n",
    "please only return 0 or 1. Return 1 for hallucination; return 0 for not hallucination.'\n",
    "\n",
    "\n",
    "instruction_dm='This is a definition modeling task. Given a standard definition of a word, and a model output definition of this word, determine if the model output is subject to hallucination.\\n\\\n",
    "{demo}\\n\\n\\\n",
    "your task:\\n\\\n",
    "standard definition: {Evidence}\\n\\\n",
    "model output definition: {Text}\\n\\n\\\n",
    "The criteria for judging are as follows:\\n\\\n",
    "Check if the model output definition is fluent and answers the question.\\n\\\n",
    "Compare the model output definition with correct examples. If inconsistencies are found or it can\\'t be inferred from the standard definition, it\\'s likely hallucination.\\n\\\n",
    "If the model output definition aligns with the standard definition or has a similar meaning, it\\'s likely not hallucination.\\n\\\n",
    "If the standard definition is \"unanswerable\" and the model output definition is \"I don\\'t know,\" it\\'s likely not hallucination.\\n\\\n",
    "please only return 0 or 1. Return 1 for hallucination; return 0 for not hallucination.'\n",
    "\n",
    "\n",
    "instruction_pg='This is a paraphrase generation task, which transforms a original sentence into a new sentence. Given a original sentence, and a model output new sentence, determine if the model output is subject to hallucination.\\n\\\n",
    "{demo}\\n\\n\\\n",
    "your task:\\n\\\n",
    "original sentence: {Evidence}\\n\\\n",
    "model output new sentence: {Text}\\n\\n\\\n",
    "The criteria for judging are as follows:\\n\\\n",
    "Check if the model output new sentence is fluent and answers the question.\\n\\\n",
    "Compare the model output new sentence with correct examples. If inconsistencies are found or it can\\'t be inferred from the original sentence, it\\'s likely hallucination.\\n\\\n",
    "If the model output new sentence aligns with the original sentence or has a similar meaning, it\\'s likely not hallucination.\\n\\\n",
    "If the original sentence is \"unanswerable\" and the model output new sentence is \"I don\\'t know,\" it\\'s likely not hallucination.\\n\\\n",
    "please only return 0 or 1. Return 1 for hallucination; return 0 for not hallucination.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# df=pd.read_json(r'C:\\Users\\刘威\\Desktop\\SHROOM\\SHROOM_dev-v1\\val.model-agnostic.json')\n",
    "df=pd.read_json(r'C:\\Users\\刘威\\Desktop\\SHROOM\\unlabeled-training-data\\train.model-agnostic.json',lines=False)\n",
    "# df = pd.read_json(r'C:\\Users\\刘威\\Desktop\\SHROOM\\SHROOM_dev-v1\\val.model-aware-selectdemo1.json', lines=True)\n",
    "\n",
    "# ['hyp', 'ref', 'src', 'tgt', 'model', 'task', 'labels', 'label', 'p(Hallucination)',\n",
    "# 'hyptop10sbert', 'srctop10sbert', 'tgttop10sbert', 'hyptop10bm25', 'srctop10bm25', 'tgttop10bm25']\n",
    "\n",
    "df = df[['tgt', 'src', 'hyp', 'task']]\n",
    "df.columns = ['tgt', 'src', 'Text', 'task']\n",
    "df['label_pre'] = ''\n",
    "\n",
    "# df = df.sample(30,random_state=40)\n",
    "# df.index=np.arange(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task\n",
       "DM    1000\n",
       "PG    1000\n",
       "MT     750\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df[df['task']=='DM']\n",
    "df1 = df1.sample(1000,random_state=42)\n",
    "\n",
    "df2=df[df['task']=='PG']\n",
    "df2 = df2.sample(1000,random_state=42)\n",
    "\n",
    "df3=df[df['task']=='MT']\n",
    "df3 = df3.sample(750,random_state=42)\n",
    "\n",
    "df = pd.concat([df1, df2, df3])\n",
    "df.index = np.arange(df.shape[0])\n",
    "df['task'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tgt'] = df['tgt'].apply(lambda x: re.sub(r\"\\s'\", \"'\", x))\n",
    "df['tgt'] = df['tgt'].apply(lambda x: re.sub(r\"\\s;\", \";\", x))\n",
    "df['tgt'] = df['tgt'].apply(lambda x: re.sub(r\"\\s:\", \":\", x))\n",
    "df['tgt'] = df['tgt'].apply(lambda x: re.sub(r\"\\s\\.\", \".\", x))\n",
    "\n",
    "df['src'] = df['src'].apply(lambda x: re.sub(r\"\\s'\", \"'\", x))\n",
    "df['src'] = df['src'].apply(lambda x: re.sub(r\"\\s;\", \";\", x))\n",
    "df['src'] = df['src'].apply(lambda x: re.sub(r\"\\s:\", \":\", x))\n",
    "df['src'] = df['src'].apply(lambda x: re.sub(r\"\\s\\.\", \".\", x))\n",
    "\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r\"\\s'\", \"'\", x))\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r\"\\s;\", \";\", x))\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r\"\\s:\", \":\", x))\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r\"\\s\\.\", \".\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DM:tgt   MT:tgt   PG:src\n",
    "\n",
    "for i in np.arange(df.shape[0]):\n",
    "\n",
    "    Text = df.loc[i, 'Text']\n",
    "\n",
    "    if df.loc[i, 'task'] == 'DM':\n",
    "        Evidence_column = 'tgt'\n",
    "        Evidence = df.loc[i, 'tgt']\n",
    "        prompt_instruction = instruction_dm\n",
    "        demo_count = 0  # number of demos\n",
    "        demo_select = 'tgttop10bm25'  # Demo selection method\n",
    "        Evidence_format='standard definition: '\n",
    "        Text_format='model output definition: '\n",
    "    elif df.loc[i, 'task'] == 'MT':\n",
    "        Evidence_column = 'tgt'\n",
    "        Evidence = df.loc[i, 'tgt']\n",
    "        prompt_instruction = instruction_mt\n",
    "        demo_count = 0  # number of demos\n",
    "        demo_select = 'tgttop10bm25'  # Demo selection method\n",
    "        Evidence_format='standard translation: '\n",
    "        Text_format='model output translation: '\n",
    "    elif df.loc[i, 'task'] == 'PG':\n",
    "        Evidence_column = 'src'\n",
    "        Evidence = df.loc[i, 'src']\n",
    "        prompt_instruction = instruction_pg\n",
    "        demo_count = 0  # number of demos\n",
    "        demo_select = 'srctop10bm25'  # Demo selection method\n",
    "        Evidence_format='original sentence: '\n",
    "        Text_format='model output new sentence: '\n",
    "    else:\n",
    "        print('error')\n",
    "        break\n",
    "\n",
    "    if demo_count == 0:\n",
    "        demo = ''\n",
    "    else:\n",
    "        demo = ''\n",
    "        demo_indexs = [index for index in eval(df.loc[i, demo_select])][:demo_count]\n",
    "        demo_index_count = 1\n",
    "        for demo_index in demo_indexs:\n",
    "            demo = demo + '\\nExample ' + str(demo_index_count) + ':'\n",
    "            demo_index_count = demo_index_count + 1\n",
    "            # demo = demo + \"\\n\" + Text_format + df.loc[demo_index, 'Text'] + \"\\n\" + Evidence_format + \\\n",
    "            #     df.loc[demo_index, Evidence_column] + \"\\nOutput: \" + str(df.loc[demo_index, 'label'])\n",
    "            demo = demo + \"\\n\" + Evidence_format + df.loc[demo_index, Evidence_column] + \"\\n\" + Text_format + \\\n",
    "                df.loc[demo_index, 'Text'] + \"\\nOutput: \" + str(df.loc[demo_index, 'label'])\n",
    "            \n",
    "        demo = demo + \"\\n\\nNext Task:\\n\"\n",
    "\n",
    "    # content = prompt_instruction + \"\\n\" + demo + \"Text: \" + Text + \"\\nEvidence: \" + Evidence\n",
    "    content = prompt_instruction\n",
    "    content = content.format(demo=demo,Evidence=Evidence,Text=Text)\n",
    "    \n",
    "\n",
    "    \n",
    "    # content = content + '\\nOnly provide 0 or 1 without any additional information.'\n",
    "\n",
    "    # print(content)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4-1106-preview\",\n",
    "                messages=[{\"role\": \"user\",\"content\": content}],\n",
    "                temperature=1.0,\n",
    "                max_tokens=400,\n",
    "                n=5\n",
    "            )\n",
    "\n",
    "            answer = [response.choices[i].message.content for i in range(len(response.choices))]\n",
    "            # answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            df.loc[i, 'label_pre'] = str(answer)\n",
    "\n",
    "            # print('第', i, '条数据:   ', df.loc[i, 'label_pre'])\n",
    "\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['tgt', 'src', 'hyp', 'task', 'label_pre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_str = df.to_json(orient='records', force_ascii=False, lines=True)\n",
    "with open('label2500_train.model-agnostic.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(df_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** 748\n",
      "*********** 1527\n",
      "*********** 1708\n",
      "*********** 1784\n",
      "*********** 1792\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "df = pd.read_json('label2500_train.model-agnostic.json', lines=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        s = row[\"label_pre\"].replace(\"'\", '\"')\n",
    "        labels = json.loads(s)\n",
    "        labels = [int(i) for i in labels if i in ['0','1']]\n",
    "\n",
    "        if not labels:\n",
    "            print(\"wrong\")\n",
    "            df.loc[index, 'label'] = 1\n",
    "        else:\n",
    "            label_counts = Counter(labels)\n",
    "            mode_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "            df.loc[index, 'label'] = mode_label\n",
    "    except Exception:\n",
    "        print(\"***********\",index)\n",
    "        df.loc[index, 'label'] = 1\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: int(x))\n",
    "\n",
    "df_json_str = df.to_json(orient='records', force_ascii=False, lines=True)\n",
    "with open('labeled_data.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(df_json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
